{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "This code-demo was digitized from the \"H2_4_Dautel_Fenton_Sosna_AAR_Demo_wCode_v4.pdf\" file attached.\n",
    "All credit goes to FERC for providing this code.\n",
    "\n",
    "WARNING: No warranty is provided, and none of the code included in this package should be used without first an engineering-review and code-quality assessment. Code is provided as-is for reference purposes only.\n",
    "\n",
    "\n",
    "\n",
    "# FERC-NOAA Hourly AAR Demonstration\n",
    "Originally coded for the 2022 FERC Technical Conference: Increasing Market and Planning Efficiency through Improved Software (Docket No. AD10-12-013)\n",
    "\n",
    "Contents: \n",
    "Steps in the Setup or Maintenance Timeframe\n",
    "Steps in Real Time to Calculate and Archive AARs\n",
    "Compute forecast margins and adjusted temperature forecasts\n",
    "Compute AARs for all the Candidate Rating Points\n",
    "Detour: AAR charts showing temp and solar intensity\n",
    "Apply the most limiting AAR line rating and update the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant Python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Load custom libraries\n",
    "## This loads the NOAA NBM data and has a pygrib dependency that only works on OSX and Linux \n",
    "import nbm as n \n",
    "\n",
    "## This contains functions for calculating line ratings based on the IEEE 738-2012 standard\n",
    "import LineRatings as lr\n",
    "\n",
    "# Set plot params\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in the Setup or Maintenance Timeframe\n",
    "## For branches rated with AARs, load their data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Branch_AAR_data, which mostly comes from the RTS-GMLC system, plus some technical data for what\n",
    "#   seem like the corresponding ACSR conductors\n",
    "df_BranchAAR = pd.read_csv('Branch_AAR_data.csv')\n",
    "\n",
    "# Also store as an array, for the more computationally intensive steps\n",
    "ar_BranchAAR = np.array(df_BranchAAR) \n",
    "df_BranchAAR.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load geographic/local data for all candidate rating points along those branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Candidate_rating_point_data, these are the points on every line segment, that we will match with \n",
    "# NBM forecasts to calculate ratings\n",
    "df_RatingPoints = pd.read_csv('Rating_Points_data.csv')\n",
    "\n",
    "# Also store as an array, to be used for the more computationally intensive steps\n",
    "ar_RatingPoints = np.array(df_RatingPoints) \n",
    "df_RatingPoints.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repackage data into exact format to be input into rating calculation function\n",
    "This takes a few minutes to run, but only needs to be run when you set up your AAR program or you change any of your line, geographic, or local data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_arguments = lr.repackage_args(df_BranchAAR, df_RatingPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time during the demo, instead of running the above command to repackage the system data, we will instead load a numpy array that has already been repackaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or save calc_arguments\n",
    "\n",
    "# To save data as new fixed .npy file:\n",
    "#np.save('calc_arguments.npy',calc_arguments)\n",
    "\n",
    "#calc_arguments_temp = np.load('calc_arguments.npy', mmap_mode='r') # to load data from .npy\n",
    "#calc_arguments = np.copy(calc_arguments_temp) # copy to new array, so not in memory map mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reate line ratings archive\n",
    "Here we create a Pandas dataframe that we will append new records to. In practice would likely be SQL or Oracle or such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe with column names\n",
    "Ratings_archive = pd.DataFrame(columns = ['AAR_cycle_Year_UTC','AAR_cycle_Month_UTC','AAR_cycle_Day_UTC',\n",
    "'AAR_cycle_Hour_UTC','line','type','fhour','rating_Amps']).astype(int)\n",
    "\n",
    "# View empty dataframe\n",
    "Ratings_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Real Time to Calculate and Archive AARs\n",
    "Now that the AAR program has been set up, we are ready at any time to load the most recent data from the NOAA National Blend of Models (NBM) forecast, and calculate a fresh set of AARs.\n",
    "### Load relevant National Blend of Models (NBM) forecast data from the NOAA/NOMADS servers\n",
    "- Files are downloaded individually from NOMADS server by calling the Temp2m() function from the NBM library imported above, which requests and downloads each file. \n",
    "- Saved files are gridded temperature forecast data, stored in grib2 format. \n",
    "- The NBM files are typically published 1 hour 10 mins after the initiation hour. Since our example code (loaded in the next step) was downloaded at 35 mins past the hour, we were able to successfully download the data published 1 hour 35 mins previously (using a previousHours = 1 parameter to our call to Temp2m() ). In some cases, you will have to use previousHours = 2. In production, transmission providers may need to test to see what the most recent available suite of forecasts is. \n",
    "- The Temp2m() function defaults to previousHours = 1, if no value is passed when the function is called. But we pass this argument explicitly below for clarity. \n",
    "- The Temp2m() (as written) also defaults to downloading data within the box bounded by -120 longitude on the left, -110 longitude on the right, 38 latitude on the top, and 31 latitude on the bottom, in order to capture the locations of the rating points in our test system. These defaults can be changed to reflect any other locations, or different values can be passed explicitly as arguments to Temp2m(). \n",
    "- The Temp2m() function also defaults to downloading all available grib files from hours 1 to 268. You can set a smaller set of files to download by specifying the forecast hours (as a list or range) in the argument forecastHourSet. Downloading a full suite of grib2 files for hours 1 through 268 can take several minutes. To demonstrate this more quickly, we'll download just for hours 1 to 70 (and then use a full set of forecasts previously downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to running this cell, manually create a folder 'NBM/' as a subfolder to your working directory\n",
    "#  if one doesn't already exist.\n",
    "\n",
    "# For the demo, we'll show downloading only grib2 files for the first 60 forecast hours.\n",
    "#initDateTime, initHour, successHours = n.Temp2m(previousHours=1,forecastHourSet=range(1,60))\n",
    "\n",
    "# Below is the \"normal\" code to download the full set of grib2 files.\n",
    "initDateTime, initHour, successHours = n.Temp2m(previousHours=1,forecastHourSet = range(1,268))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment for presentation, loading previous forecasts\n",
    "Since we'll use previously downloaded data, we'll now delete the files we just downloaded and copy the older files to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear files from saveDirectory\n",
    "#for files in os.listdir('NBM/'):\n",
    "#    path = os.path.join('NBM/', files)\n",
    "#    try:\n",
    "#        shutil.rmtree(path)\n",
    "#    except OSError: #if rmtree() command doesn't work on local OS\n",
    "# os.remove(path)\n",
    "#os.rmdir('NBM/')\n",
    "\n",
    "# Copy previously downloaded files to NBM directory\n",
    "#shutil.copytree('NBM_demo', 'NBM/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variables that would have been created if we had run Temp2m() instead of just uploading our data.\n",
    "# These are needed in the line rating calculations below.\n",
    "\n",
    "#initDateTime = datetime(2022, 6, 19, 14, 25, 26, 431672) # UTC date/time at which we downloaded our data\n",
    "#initHour = 14\n",
    "#successHours = list(range(1,38)) + list(range(40,191,3)) + list(range(196,263,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process downloaded grib2 files\n",
    "Process downloaded grib2 files by:\n",
    "- Extracting grib2 data into numpy arrays \n",
    "- For each forecast hour (1 to 240), associating each candidate rating point with its nearest forecast point. \n",
    "- Compute forecast margin and apply to forecast. Not wanting to make any suggestions about what an appropriate forecast margin is, we implement a forecast margin that is a simple multiple of the standard deviation (where our multiple was selected at random from a range of candidates). \n",
    "- A note: the standard deviations in the files do not represent forecast accuracy relative to observed data but instead repersent how closely the different models in the NBM agree with each other on a given forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NBM i,j points into array that can be passed to n.processNBMGribFiles()\n",
    "ar_RatingPointsIJs = np.array(df_RatingPoints[['NBM_i', 'NBM_j']])\n",
    "\n",
    "temp, tempSTD = n.process_nbm_grib_files(initHour, successHours, ar_RatingPointsIJs)\n",
    "\n",
    "# Display first row of temp, showing all 240 forecasted temps for that rating point\n",
    "temp[0],temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute forecast margins and adjusted temperature forecasts\n",
    "\n",
    "How to calculate forecast margins and what confidence levels to reflect are an area for more research and discussion. For purposes of having numbers to plot, we implemented an approach, but we do not believe our approach is methodolgically valid. In general, our method reflects published annual average mean absolute errors for temperature forecasts over similar timeframes, and then scales those values up or down depending on the reported values for forecast standard deviation (which is a measure of how well the underlying NBM models agree, not a proxy for standard error of the forecast itself). That yields an estimated standard error, which we then scale up by 3.4 (a randomly chosen multiple) in order to achieve some unstated confidence level.\n",
    "\n",
    "This relies on an assumption that the forecast errors are normally distributed. However, we know from weather experts and published annual average mean absolute errors for temperature forecasts that this is an erroneous assumption.\n",
    "\n",
    "How to more accurately estimate forecast margins is an important consideration for implementing AARs and should be included in further research/discussion.\n",
    "\n",
    "Below, we calculate estimated standard error, using a blackbox function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function was created only for the purposes of being able to compute values for demonstration,\n",
    "but this function is not methodologcially valid, and should not be used for any purposes other\n",
    "than demonstration.\n",
    "\"\"\"\n",
    "def estForecastStdErr(STD,fhour):\n",
    "\n",
    "    # Compute an estimate for standard error based purely on forecast hour\n",
    "    #   (this is designed to roughly match data published for a different but somewhat similar forecast)\n",
    "    stdErr = fhour * 0.035 + 0.5\n",
    "\n",
    "    # Scale the stdErr up or down depending on how well the models underlying the NBM forecast\n",
    "    #   agree with each other (as measured by the tempSTD returned from the NBM data).\n",
    "    scale = STD*0.06 + 0.3\n",
    "\n",
    "    return stdErr*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply the estimated forecast STD error function, again for demonstration purposes only\n",
    "estStdErr = np.empty(shape=tempSTD.shape) # create empty array for estimated standard error of forecast\n",
    "estStdErr[:] = np.nan\n",
    "\n",
    "# Compute estimated standard error using the estForecastStdErr() function\n",
    "for fhour in range(0,tempSTD.shape[1]): \n",
    "    estStdErr[:,fhour] = estForecastStdErr(tempSTD[:,fhour],fhour)\n",
    "\n",
    "estStdErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastMargin = np.empty(shape=tempSTD.shape) # create empty array for forecast margin\n",
    "forecastMargin[:] = np.nan\n",
    "\n",
    "forecastMargin = estStdErr*3.4\n",
    "# Show forecastMargin and shape\n",
    "(forecastMargin, forecastMargin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 35 series (every 100 of the 3513 rows) in forecastMargin to plot as scatter vs forecast hour.\n",
    "for row_num in range(0,forecastMargin.shape[0],100):\n",
    "    x = range(forecastMargin.shape[1]) # create x values as the forecast hours\n",
    "    y = forecastMargin[row_num,:]*9/5 # create y values as forecast margins converted from C intervals to F intervals\n",
    "    sns.scatterplot(x = x,y = y,size=1,legend=False)\n",
    "\n",
    "plt.xlabel(\"forecast hour\"); plt.ylabel(\"forecast margin (F)\"); plt.gca().set_ylim(bottom=0)\n",
    "plt.title('Forecast margin vs forecast hour for 35 candidate rating points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the adjusted temperature (which incorporates the forecast margin) \n",
    "# This will be used to calculate the AARs\n",
    "adjustedTemp = temp + forecastMargin\n",
    "\n",
    "# Show values and shape\n",
    "(adjustedTemp, adjustedTemp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we examine the different temperature plots at a randomly selected candidate rating point, across the 240 forecast hours.\n",
    "\n",
    "For reference, we also show a straight line for T = 105F, which is a common temperature assumption for static ratings or for summer seasonal ratings.\n",
    "\n",
    "Note the impact of the forecast margin on the adjusted temperatures, which are especially high and diverge from the expected temperatures toward the end of the timeframe. AARs based on these adjusted temperatures would lead to less transfer capacity on the hottest days in this period. If the forecast margin were accurate these would be the right ratings to use, however, if the forecast margin relies of false assumptions like normally distributed temperature errors, then this would be an arbitrary loss of transfer capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecast margin, expected temp, adjusted temp, and T = 105F(a typical summer assumption)\n",
    "plt.plot(temp[0]*9/5+32) #the unadjusted temp forecast (converted to F)\n",
    "plt.text(200,75,'expected temp', color = 'xkcd:cerulean')\n",
    "\n",
    "plt.plot(forecastMargin[0]*9/5) # forecast margin (converted to an F interval)\n",
    "plt.text(50,23,'forcast margin', color = 'xkcd:emerald green')\n",
    "\n",
    "plt.plot(adjustedTemp[0]*9/5+32) # adjusted temp (converted to F)\n",
    "plt.text(120,120,'adjusted temp', color = 'xkcd:scarlet')\n",
    "\n",
    "plt.plot([0,260],[105,105], linestyle= 'dashed') # for reference, show a typical summer seasonal rating\n",
    "plt.xlabel(\"forecast hour\"); plt.ylabel(\"temp (F)\")\n",
    "\n",
    "plt.title('Temp adjustment for selected candidate rating point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AARs for all the Candidate Rating Points\n",
    "### Prepare arguments for calculation\n",
    "\n",
    "Append the downloaded weather data into the prevously prepared arguments.\n",
    "\n",
    "The user (or the automated system, in practice) will need to choose how many forecast hours to calculate line ratings for (and therefore which columns in the adjustedTemp array to include as arguments for the calculation). In our case we have set up the calc_arguments array such that it will only hold 240 hours.\n",
    "\n",
    "In this step, we also exclude column 0 from the adjustedTemp array, which means we are excluding forecast hour f001. We do this because f001 is already in the past by the time we run our calculation.\n",
    "\n",
    "Thus, for the purposes of the line rating calculation, what was f002 becomes our first forecast hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the 240 temperature forecasts after hour f001.\n",
    "lr.update_args(calc_arguments, adjustedTemp[:,1:241], initDateTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the calculation of ratings at each candidate rating point\n",
    "Each argument for the calculate_thermal_rating() function is passed as a column of the calc_arguments array.\n",
    "\n",
    "We write the output of the calculate_thermal_rating() function to the last column of calc_arguments (which will be further used as an argument further below for determining the overal ratings for each line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_arguments[:,-1] = lr.calculate_thermal_rating(\n",
    "T_a = calc_arguments[:,4],\n",
    "T_s = calc_arguments[:,5],\n",
    "D = calc_arguments[:,6],\n",
    "emis = calc_arguments[:,7],\n",
    "absorp = calc_arguments[:,8],\n",
    "R = calc_arguments[:,9],\n",
    "W_v = calc_arguments[:,10],\n",
    "W_a = calc_arguments[:,11],\n",
    "elev = calc_arguments[:,12],\n",
    "air_qual = calc_arguments[:,13],\n",
    "year = calc_arguments[:,14],\n",
    "day_of_year = calc_arguments[:,15],\n",
    "start_hour = calc_arguments[:,16],\n",
    "latitude = calc_arguments[:,17],\n",
    "longitude = calc_arguments[:,18],\n",
    "time_zone = calc_arguments[:,19],\n",
    "line_azimuth = calc_arguments[:,20],\n",
    "verbose=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: AAR charts showing temp and solar intensity\n",
    "### Show calculated candidate ratings along one line during first forecast hour\n",
    "Here you see the AAR ratings at all the candidate rating points for a given rating line and forecast hour. Only the lowest rating will be applied to the entire line and archived. The relationship between normal and emergency rating types is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_LINE = 65\n",
    "EXAMPLE_HOUR = 1\n",
    "\n",
    "# Create dataframe with columns for line (0), point (1), type (2), forecast hour (3), and candidate rating (21).\n",
    "df = pd.DataFrame(calc_arguments[:,[0,1,2,3,21]],\n",
    "columns = ['line','point','type','fhour','rating']\n",
    ")\n",
    "# masks to filter data\n",
    "mask = (df['line'] == EXAMPLE_LINE) & (df['fhour'] == EXAMPLE_HOUR)\n",
    "# Reshape df to facilitate plotting\n",
    "df_reshaped = df[mask].pivot(index='point', columns='type', values='rating')\n",
    "\n",
    "df_reshaped.plot(title='Candidate Ratings for line 65, forecast f001')\n",
    "plt.legend(['Normal', 'Long-term emergency', 'Short-term emergency'])\n",
    "plt.ylabel(\"Rating (Amps)\")\n",
    "plt.xlabel('Candidate rating point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show calculated candidate normal ratings for one line from early morning to noon\n",
    "Note how the AAR ratings decline from 5am to 2pm for a given rating line and forecast hour, which is as expected as temperatures rise and solar heating intensifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_LINE = 65\n",
    "EXAMPLE_TYPE = 0 #0 for normal ratings\n",
    "\n",
    "# Define new column which will have local time\n",
    "#  (valid for our fhour range of 16 to 40)\n",
    "df['local_time'] = df['fhour']-16\n",
    "\n",
    "# Create masks to filter data\n",
    "mask = (((df['line'] == EXAMPLE_LINE) & (df['type'] == EXAMPLE_TYPE)) & (df['local_time'] <= 14)) & (df['local_time'] >= 5)\n",
    "\n",
    "# Reshape df to facilitate plotting\n",
    "df_reshaped = df[mask][['point','local_time','rating']].pivot(index='point', \n",
    "columns='local_time', \n",
    "values='rating')\n",
    "colors = cm.cool(np.linspace(0, 1, 31-21))\n",
    "df_reshaped.plot(title='Candidate normal ratings for line 65\\nfrom 5am to 2pm local time',color = colors)\n",
    "\n",
    "plt.ylabel('Rating (Amps)')\n",
    "plt.xlabel('Candidate rating point')                                       # places legend on side\n",
    "plt.legend(['5am','6am','7am','8am','9am','10am','11am','noon','1pm','2pm'],bbox_to_anchor = (1, 1)) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show calculated rating for one candidate rating point across a 24-hour cycle, along with ambient temp\n",
    "As expected, there is an inverse relationship between the temperature and the AAR rating for a given candidate rating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_LINE = 65\n",
    "\n",
    "# Get new dataframe (like the previous) that adds a column for T_a (air temp) (column 4)\n",
    "df = pd.DataFrame(calc_arguments[:,[0,1,2,3,4,21]],\n",
    "columns = ['line','point','type','fhour','T_a','rating']\n",
    ")\n",
    "\n",
    "# Define new column which will have local time\n",
    "#  (valid for our fhour range of 16 to 40)\n",
    "df['local_time'] = df['fhour']-16\n",
    "\n",
    "# Create masks to filter the data\n",
    "mask1 = (df['line'] == EXAMPLE_LINE)\n",
    "mask2 = (df['local_time'] <= 24) & (df['local_time'] >= 0)\n",
    "mask3 = df['point'] == 0\n",
    "mask4 = df['type'] == 0 #normal\n",
    "mask = ((mask1 & mask2) & mask3) & mask4\n",
    "\n",
    "# Plot rating and T_a vs local time\n",
    "ax = df[mask].plot(x='local_time', y = ['rating','T_a'], secondary_y = 'T_a')\n",
    "\n",
    "ax.set_ylabel('Rating (Amps)')\n",
    "ax.set_title('Rating and air temp across a day')\n",
    "ax.right_ax.set_ylabel('Air Temp (C)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to determine from that chart how much of the rating change is due to ambient temperature change (which is clearly correlated) and how much is due to changes in solar heating across the day.\n",
    "\n",
    "So below we plot rating for the same candidate rating point with the ambient air temperature and all other conditions constant at local hour 10, except that we let solar heating vary across the 24\n",
    "hours of a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks to filer the data\n",
    "mask1 = df['line'] == 65\n",
    "mask2 = df['fhour']== 10+16\n",
    "mask3 = df['point'] == 0\n",
    "mask4 = df['type'] == 0 #normal\n",
    "mask = ((mask1 & mask2) & mask3) & mask4\n",
    "\n",
    "x = calc_arguments[mask] # filter data\n",
    "\n",
    "# Calculate line ratings for the filtered data point, except for start_hour, which can vary from 0 to 23\n",
    "y = lr.calculate_thermal_rating(\n",
    "T_a = x[:,4],\n",
    "T_s = x[:,5],\n",
    "D = x[:,6],\n",
    "emis = x[:,7],\n",
    "absorp = x[:,8],\n",
    "R = x[:,9],\n",
    "W_v = x[:,10],\n",
    "W_a = x[:,11],\n",
    "elev = x[:,12],\n",
    "air_qual = x[:,13],\n",
    "year = x[:,14],\n",
    "day_of_year = x[:,15],\n",
    "start_hour = np.array(range(24)),\n",
    "latitude = x[:,17],\n",
    "longitude = x[:,18],\n",
    "time_zone = x[:,19],\n",
    "line_azimuth = x[:,20],\n",
    "verbose=0, \n",
    ")\n",
    "\n",
    "# Fill area\n",
    "new_x = range(4, 20)\n",
    "new_y_low = 1130\n",
    "new_y_high = y[4:20]\n",
    "plt.fill_between(new_x, new_y_low, new_y_high , color = 'yellow', alpha=0.2)\n",
    "\n",
    "plt.ylim([1100,1280])\n",
    "plt.plot(y, 'o-')\n",
    "xvals = [0,4,4,19,19,24]\n",
    "yvals = [y[0], y[0], y[12], y[12], y[0], y[0]]\n",
    "plt.plot(xvals, yvals)\n",
    "plt.ylabel('Rating (Amps)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.title('Rating across day (holding air temperature constant)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of detour\n",
    "\n",
    "## Apply the most limiting AAR line rating and update the archive\n",
    "For each line, compute its line rating for a given rating type and forecast hour as the minimum of all of the candidate line ratings for that rating type and forecast hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the minimum rating per line, per type, per forecast hour\n",
    "unique_ratings_per_AAR_cycle = df.groupby(['line', 'type','fhour'])['rating'].min().reset_index().astype(int)\n",
    "\n",
    "unique_ratings_per_AAR_cycle = unique_ratings_per_AAR_cycle.rename(columns = {'rating':'rating_Amps'}) \n",
    "\n",
    "# Add the AAR cycle date/time info\n",
    "## Based on having started the AAR calculation cycle and requested a NBM forecast 1 hour prior to the\n",
    "#   request hour.  Would need to adjust the below line if ever used a different amount of time (say 2 hours,\n",
    "#   maybe if the data for 1 hour previously wasn't available due to a server error) prior to request hour.\n",
    "AAR_cycle_UTC = initDateTime + timedelta(hours=1) \n",
    "\n",
    "# Record the line ratings and other metadata in a small array\n",
    "unique_ratings_per_AAR_cycle = unique_ratings_per_AAR_cycle.join(pd.DataFrame(\n",
    "    {'AAR_cycle_Year_UTC': AAR_cycle_UTC.strftime('%Y'),\n",
    "    'AAR_cycle_Month_UTC': AAR_cycle_UTC.strftime('%m'),\n",
    "    'AAR_cycle_Day_UTC': AAR_cycle_UTC.strftime('%d'),\n",
    "    'AAR_cycle_Hour_UTC': AAR_cycle_UTC.strftime('%H')},\n",
    "    index=df.index))\n",
    "\n",
    "unique_ratings_per_AAR_cycle.head()  # view values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point in our demo, ratings have been calculated for the specific instants at the top of each hour that the NBM forecasts apply to. To produce a useable and reliable line rating across each hourly period, we set each such period rating equal to the lesser of the instant ratings calculated for the start and end if each hourly period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the rating at the top of hour t with the rating at the top of hour t+1\n",
    "## and take the lowest, which will become the rating column\n",
    "updated_rating_Amps = []\n",
    "i = 0\n",
    "\n",
    "for i in range(0, len(unique_ratings_per_AAR_cycle['rating_Amps'])):\n",
    "    updated_rating_Amps.append(min(unique_ratings_per_AAR_cycle['rating_Amps'][i:(i+2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the share of hourly ratings we update in the conservative column\n",
    "count_dif = sum(map(lambda x,y: bool(x-y),unique_ratings_per_AAR_cycle['rating_Amps'],updated_rating_Amps))\n",
    "\n",
    "# 39% of them changed\n",
    "(count_dif/len(unique_ratings_per_AAR_cycle['rating_Amps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the updated ratings to the hourly rating dataframe\n",
    "unique_ratings_per_AAR_cycle.insert(4,'updated_rating_Amps',updated_rating_Amps)\n",
    "\n",
    "# And look at them, columns 4 and 5:\n",
    "unique_ratings_per_AAR_cycle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the first 24h for the top of the hour and conservative ratings\n",
    "plt.plot(unique_ratings_per_AAR_cycle['rating_Amps'].loc[(\n",
    "                                            unique_ratings_per_AAR_cycle['line']==0)& \n",
    "                                            (unique_ratings_per_AAR_cycle['type']==0)][0:24]) \n",
    "\n",
    "plt.plot(unique_ratings_per_AAR_cycle['updated_rating_Amps'].loc[(\n",
    "                                            unique_ratings_per_AAR_cycle['line']==0)& \n",
    "                                            (unique_ratings_per_AAR_cycle['type']==0)][0:24], color = 'xkcd:tangerine') \n",
    "\n",
    "plt.legend(['rating_Amps', 'updated_rating_Amps']) \n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Rating (Amps)')\n",
    "plt.title('Top-of-the-hour vs hourly duration ratings')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will replace the updated ratings (based on the lower of the t or t+1 ratings) \n",
    "## with the original top of the hour ratings before we use them and update the archive\n",
    "unique_ratings_per_AAR_cycle['rating_Amps'] = unique_ratings_per_AAR_cycle['updated_rating_Amps']\n",
    "unique_ratings_per_AAR_cycle.drop('updated_rating_Amps', axis = 1, inplace = True)\n",
    "unique_ratings_per_AAR_cycle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the archive\n",
    "Ratings_archive = pd.concat([Ratings_archive, unique_ratings_per_AAR_cycle],ignore_index = True)\n",
    "\n",
    "Ratings_archive.head() # View values"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
